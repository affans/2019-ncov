---
title: "2019ncov Underreporting Analysis"
author: "Affan Shoukat"
date: '2020-02-05'
output: pdf_document
geometry: margin=0.3in
header-includes:
   - \usepackage{bm}
   - \usepackage[makeroom]{cancel}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction 
It is suspected that confirmed cases of the 2019-nCov out of Wuhan, China are under-reported, i.e. the reported value is less than the true value. This can be due to a lack of robust surveillance network, diagnostic testing, and government. If data is under-reported, inference from the observed counts will be biased and consequently, poorly informed decisions. The bias will affect parameter estimates, predictions, and associated uncertainty. 

We use a statistical model, in the Bayesian framework, to investigate and account for the under-reporting^[under-reporting is different from the concept of missing data introduced by Little/Rubin] in confirmed cases from Wuhan, China. 

## Background
Let $y_t$ be the true number of cases. If counts are have been perfectly observed, they can be modelled by an appropriate distribution $p(y_t \mid \mathbf{\theta})$, usually either Poisson or Neg Binomial. The $\theta$ represents random effects (i.e. various dependencies and space/time structures, as well as parameters associated with the covariates. Inference is then based on the conditional likelihood of data (assuming independence in $y$): 
\begin{equation}
  p(\mathbf{y} \mid \pmb{\theta}) = \prod_t p(y_t \mid \theta)
\end{equation}
Now suppose these true counts are not observed, and instead we observe under-reported counts $z_t < y_t$. This implies using the above equation for observed counts will lead to biased inference. A common approach to handling under-reported data is *censored likelyhood*, however this method lacks a way of quantifying the severity of under-reporting. Instead, we use the *Hierarchical Count Framework*. A general description of this methodology is given in Stoner et al, Dvorzak et al.

## Hierachical Count Framework 
We assume the true, but unobserved counts $y_t$ is generated by a Possion process with rate $\lambda$. Suppose a case is reported with reporting probability $\pi_t$ and that reporting of counts is independent conditional on the reporting probability. Then, the observed (reported) counts $z_t$ are generated from the true counts $y_t$ by Binomial thinning, i.e. $z_t$ is assumed a Binomial realization out of $y_t$, with probability $\pi_t$. The basic form of the model is given by 
\begin{align}
  z_{t} \mid y_t, \gamma_t &\sim \text{Binomial}(\pi_t, y_t) \\
  \log\left( \frac{\pi_t}{1 - \pi_t} \right) &= \beta_0 + \beta_1 w_t + \gamma_t \\ 
  y_t \mid \lambda_t &\sim \text{Poisson}(\lambda_t) \\
  \log(\lambda_t) &= \cancel{\log(P)} + \alpha_0 + \alpha_1 x_t
\end{align}
The reporting probability $\pi_t$ and Poisson rate parameter $\lambda_t$ depend on a set of covariates. The under-reporting covariate $w_t$ (i.e. variable that potentially explains the under-reporting) enters the model through a linear predictor in the logistic transformation of $\pi_t$. Another interpretation of $\pi_t$ is the severity of the under-reporting. The true counts $y_t$ are modelled as a latent Poisson variable with mean $\lambda_t$, characterized at the log-scale as a linear combination of the $x_t$ covariates associated with the process giving rise to the counts. In the paper by [stoner], they have a $\log(P)$ term, which is an offset such as population counts. I do not think we need this term for our study. The term $\gamma_t$ is an additional unstructured $N(0, \epsilon^2)$ effect to allow for the effect of potential unobserved covariates for the under-reporting of cases. 

Our goal is to estimate the parameters $\alpha_0, \alpha_1, \beta_0, \beta_1, \pi_t, \lambda_t$. Since the true counts $y_t$ are not known, inference has to be made on the observed counts $z_t$. By using Bayes rule and integration, it can be shown the observed counts $z_t$ are Poisson distributed, i.e. $$ z_t \sim \text{Poisson}(\pi_t \lambda_t) $$ By using this equation, the MCMC is much more efficient, and samples of $\mathbf{y}$ can be sampled using $y_t - z_t \sim \text{Poisson}((1 - \pi_t)\lambda_t)$. This means a complete predictive inference on the true counts $y_t$ is possible, however this simplification suggests the observed counts $z_t$ could arise from either high $\pi_t$, low $\lambda$ or vice versa. There is a lack of identifibility; in particulaar, there is a lack of identifibility of the intercepts $\alpha_0$ and $\beta_0$.  

## Analysis
Load the relevant packages:
```{r, echo=T, results = 'hide', message=FALSE}
rm(list = ls())
library(ggplot2) # For reproducing plots seen in the paper.
library(nimble) # For MCMC computation using NIMBLE.
library(coda) # For manipulation of MCMC results.
library(mgcv)
library(dplyr)
library(viridis)
```
Lets load the data (daily confirmed counts and suspected cases), covariates, and basic variables
```{r}
suspectedcases = c(54, 37, 393, 1072, 1965, 2684, 4794, 6973, 9239, 12167, 15238, 17988)
totalcases = c(77, 149, 131, 259, 444, 688, 769, 1771, 1459, 1737, 1982, 2102)

xcov = c(0.58778626, 0.801075269, 0.25, 0.194590533, 0.184308842, 0.204033215, 0.138234765,	0.202538884, 0.136380632, 0.124928078, 0.115098722, 0.104629169)
wcov = c(0.1, 0.12, 0.14, 0.16, 0.18, 0.2, 0.22, 0.24, 0.26, 0.28, 0.3, 0.32)
# center the covariates
xcov = xcov-mean(xcov)
wcov = wcov-mean(wcov)

UR_N=length(totalcases) # Number of observations.
```
We use the R package `nimble` to define our model: 
```{r}
# Model code.
UR_code=nimbleCode({ 
  for(i in 1:n){
    pi[i] <- ilogit(b0 + b1*w[i] + gamma[i])
    lambda[i] <- exp(a0 + a1*x[i])
    z[i] ~ dpois(pi[i]*lambda[i])
    gamma[i]~dnorm(0,sd=epsilon)
  }
  a0 ~ dnorm(-8,sd=1)    ## TB paper: unlikely high case counts over a million
  a1 ~ dnorm(0,sd=10)    ## uninformative
  b0 ~ dnorm(2,sd=0.6)   ## informative prior, 
  b1 ~ dnorm(0,sd=10)    ## uninformative
  
  epsilon ~ T(dnorm(0,1),0,)  ## truncated prior normal distribution 
})
```
Next, we build the model with 4 chains, all with different initial conditions. We do this to to check for MCMC convergence and to compute the PSRF.
```{r}
# Set up data for NIMBLE.
UR_constants=list(n=UR_N, offset=offset, x=xcov,  w=wcov)
UR_data=list(z=totalcases)
# Set initial values.
UR_inits1=list(epsilon=0.25,a0=-7, a1=-0.1, b0=1.8, b1=0.1, gamma=rnorm(UR_N,0,0.25))
UR_inits2=list(epsilon=0.5,a0=-7, a1=-0.1, b0=2.2, b1=0.1, gamma=rnorm(UR_N,0,0.25))
UR_inits3=list(epsilon=0.25,a0=-7, a1=0.1, b0=1.8, b1=-0.1, gamma=rnorm(UR_N,0,0.25))
UR_inits4=list(epsilon=0.5,a0=-7, a1=0.1, b0=2.2, b1=-0.1, gamma=rnorm(UR_N,0,0.25))
UR_inits=list(chain1=UR_inits1, chain2=UR_inits2, chain3=UR_inits3, chain4=UR_inits4)

# Build the model.
UR_model <- nimbleModel(UR_code, UR_constants, UR_data, UR_inits)
UR_compiled_model <- compileNimble(UR_model, resetFunctions = TRUE)
```
Set the MCMC samplers and monitor the parameters/variables of interest
```{r}
UR_mcmc_conf <- configureMCMC(UR_model, monitors=c('a0', 'a1', 'b0', 'b1', 'epsilon', 'pi','lambda'),useConjugacy = TRUE)

UR_mcmc <- buildMCMC(UR_mcmc_conf)
UR_compiled_mcmc <- compileNimble(UR_mcmc, project = UR_model, resetFunctions = TRUE)
```